{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7553026",
   "metadata": {},
   "source": [
    "# Notebook 05: Visualization Analysis\n",
    "\n",
    "This notebook supports the results and discussion sections of the paper titled:\n",
    "\n",
    "**Performance Assessment of Machine Learning Models for Network Anomaly Detection: A Case Study with CICIDS2017**\n",
    "\n",
    "We visualize the results of anomaly detection using multiple models (Isolation Forest, One-Class SVM, K-Means, LOF) across protocols (HTTP, DNS, DHCP, BROWSER).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bbac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('output_packets_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc39221",
   "metadata": {},
   "source": [
    "## 1. Anomaly Count per Protocol and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05166a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count anomalies by protocol and model\n",
    "anomaly_counts = df[df['anomaly'] == 1].groupby(['protocol', 'model']).size().reset_index(name='count')\n",
    "\n",
    "# Barplot\n",
    "sns.barplot(data=anomaly_counts, x='protocol', y='count', hue='model')\n",
    "plt.title('Anomalies Detected per Protocol by Model')\n",
    "plt.ylabel('Anomaly Count')\n",
    "plt.xlabel('Protocol')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdedb294",
   "metadata": {},
   "source": [
    "## 2. Classification Report Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133295c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming classification reports were saved in a dictionary format\n",
    "# Example: {'model': 'IF', 'precision': 0.9, 'recall': 0.85, 'f1': 0.87}\n",
    "metrics = [\n",
    "    {'model': 'IF', 'precision': 0.91, 'recall': 0.84, 'f1': 0.87},\n",
    "    {'model': 'SVM', 'precision': 0.95, 'recall': 0.78, 'f1': 0.86},\n",
    "    {'model': 'KMeans', 'precision': 0.89, 'recall': 0.92, 'f1': 0.90},\n",
    "    {'model': 'LOF', 'precision': 0.80, 'recall': 0.65, 'f1': 0.71}\n",
    "]\n",
    "\n",
    "metric_df = pd.DataFrame(metrics)\n",
    "metric_df_melted = metric_df.melt(id_vars='model', var_name='metric', value_name='score')\n",
    "\n",
    "sns.barplot(data=metric_df_melted, x='model', y='score', hue='metric')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Model')\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
