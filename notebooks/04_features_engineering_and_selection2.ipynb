{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osahonokoro/anomaly-detection-isolationforest-cicids2017/blob/main/notebooks/04_features_engineering_and_selection2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f8b673ca-f40e-4972-a62c-63130888673b",
      "metadata": {
        "id": "f8b673ca-f40e-4972-a62c-63130888673b",
        "outputId": "41aed39d-1ac4-46f9-82e9-7bfbf508ffa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… Reloaded dataset. Shape: (2827876, 82)\n",
            "binary_anomaly\n",
            "0    2771314\n",
            "1      56562\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# âœ… Mount Google Drive first (run this only once after reconnecting)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# âœ… Load the anomaly-labeled dataset from your Drive\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/anomaly_detection_output.csv')\n",
        "df.columns = df.columns.str.strip()\n",
        "df['binary_anomaly'] = df['anomaly'].map({'Normal': 0, 'Anomaly': 1})\n",
        "\n",
        "print(\"âœ… Reloaded dataset. Shape:\", df.shape)\n",
        "print(df['binary_anomaly'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  One-Class SVM with Chunked Processing (100,000 rows per batch)\n",
        "\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Parameters\n",
        "chunk_size = 100_000\n",
        "svm_preds = []\n",
        "binary_labels = []\n",
        "\n",
        "# Drop non-feature columns\n",
        "base_features = df.drop(columns=['anomaly', 'binary_anomaly'], errors='ignore')\n",
        "\n",
        "# Loop over chunks\n",
        "for i in range(0, len(df), chunk_size):\n",
        "    chunk = df.iloc[i:i+chunk_size].copy()\n",
        "    features = chunk.drop(columns=['anomaly', 'binary_anomaly'], errors='ignore')\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    scaled = scaler.fit_transform(features)\n",
        "\n",
        "    # Train One-Class SVM on the chunk\n",
        "    svm = OneClassSVM(nu=0.02, kernel='rbf', gamma='scale')\n",
        "    pred = svm.fit_predict(scaled)\n",
        "\n",
        "    # Convert predictions to readable labels\n",
        "    pred = pd.Series(pred).map({1: 'Normal', -1: 'Anomaly'})\n",
        "    svm_preds.extend(pred)\n",
        "\n",
        "    # Store true labels\n",
        "    binary_labels.extend(chunk['binary_anomaly'])\n",
        "\n",
        "    print(f\"âœ… Processed rows {i} to {i+chunk_size-1}\")\n",
        "\n",
        "# Final Evaluation\n",
        "print(\"\\nðŸ“Š One-Class SVM Chunked Evaluation:\")\n",
        "print(pd.Series(svm_preds).value_counts())\n",
        "print(\"\\nðŸ“ˆ Classification Report:\")\n",
        "print(classification_report(binary_labels, pd.Series(svm_preds).map({'Normal': 0, 'Anomaly': 1})))\n"
      ],
      "metadata": {
        "id": "4WbqMH9tMvFi",
        "outputId": "da67c428-4eb5-4db2-8b3c-9a992282e83a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4WbqMH9tMvFi",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Processed rows 0 to 99999\n",
            "âœ… Processed rows 100000 to 199999\n",
            "âœ… Processed rows 200000 to 299999\n",
            "âœ… Processed rows 300000 to 399999\n",
            "âœ… Processed rows 400000 to 499999\n",
            "âœ… Processed rows 500000 to 599999\n",
            "âœ… Processed rows 600000 to 699999\n",
            "âœ… Processed rows 700000 to 799999\n",
            "âœ… Processed rows 800000 to 899999\n",
            "âœ… Processed rows 900000 to 999999\n",
            "âœ… Processed rows 1000000 to 1099999\n",
            "âœ… Processed rows 1100000 to 1199999\n",
            "âœ… Processed rows 1200000 to 1299999\n",
            "âœ… Processed rows 1300000 to 1399999\n",
            "âœ… Processed rows 1400000 to 1499999\n",
            "âœ… Processed rows 1500000 to 1599999\n",
            "âœ… Processed rows 1600000 to 1699999\n",
            "âœ… Processed rows 1700000 to 1799999\n",
            "âœ… Processed rows 1800000 to 1899999\n",
            "âœ… Processed rows 1900000 to 1999999\n",
            "âœ… Processed rows 2000000 to 2099999\n",
            "âœ… Processed rows 2100000 to 2199999\n",
            "âœ… Processed rows 2200000 to 2299999\n",
            "âœ… Processed rows 2300000 to 2399999\n",
            "âœ… Processed rows 2400000 to 2499999\n",
            "âœ… Processed rows 2500000 to 2599999\n",
            "âœ… Processed rows 2600000 to 2699999\n",
            "âœ… Processed rows 2700000 to 2799999\n",
            "âœ… Processed rows 2800000 to 2899999\n",
            "\n",
            "ðŸ“Š One-Class SVM Chunked Evaluation:\n",
            "Normal     2771051\n",
            "Anomaly      56825\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ðŸ“ˆ Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99   2771314\n",
            "           1       0.41      0.42      0.41     56562\n",
            "\n",
            "    accuracy                           0.98   2827876\n",
            "   macro avg       0.70      0.70      0.70   2827876\n",
            "weighted avg       0.98      0.98      0.98   2827876\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f481837e-c42f-4a06-b625-527814e178b1",
      "metadata": {
        "id": "f481837e-c42f-4a06-b625-527814e178b1",
        "outputId": "b5922626-e1b9-4354-a47d-e107b56cd6e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Œ Smallest cluster assumed to be anomaly: Cluster 1\n",
            "\n",
            "ðŸ“Š K-Means Anomaly Detection Results:\n",
            "kmeans_anomaly\n",
            "Normal     2507971\n",
            "Anomaly     319905\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# K-Means with Chunked Processing (500,000 rows per batch)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set paths\n",
        "input_file = '/content/drive/MyDrive/anomaly_detection_output.csv'\n",
        "chunk_size = 500_000\n",
        "\n",
        "# Placeholder for results\n",
        "kmeans_results = []\n",
        "\n",
        "# First, fit the scaler on a small subset\n",
        "sample_df = pd.read_csv(input_file, nrows=100_000)\n",
        "sample_df.columns = sample_df.columns.str.strip()\n",
        "base_sample = sample_df.drop(columns=['anomaly', 'binary_anomaly'], errors='ignore')\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(base_sample)\n",
        "\n",
        "# Fit KMeans on the same subset to get initial clusters\n",
        "scaled_sample = scaler.transform(base_sample)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "kmeans.fit(scaled_sample)\n",
        "\n",
        "# Find the smallest cluster (used as anomaly class)\n",
        "cluster_counts = Counter(kmeans.labels_)\n",
        "smallest_cluster = min(cluster_counts, key=cluster_counts.get)\n",
        "print(f\"ðŸ“Œ Smallest cluster assumed to be anomaly: Cluster {smallest_cluster}\")\n",
        "\n",
        "# Process in chunks\n",
        "for chunk in pd.read_csv(input_file, chunksize=chunk_size):\n",
        "    chunk.columns = chunk.columns.str.strip()\n",
        "    features = chunk.drop(columns=['anomaly', 'binary_anomaly'], errors='ignore')\n",
        "\n",
        "    # Scale\n",
        "    scaled_chunk = scaler.transform(features)\n",
        "\n",
        "    # Predict clusters\n",
        "    chunk_clusters = kmeans.predict(scaled_chunk)\n",
        "\n",
        "    # Mark anomalies\n",
        "    chunk['kmeans_anomaly'] = ['Anomaly' if label == smallest_cluster else 'Normal' for label in chunk_clusters]\n",
        "\n",
        "    # Store results\n",
        "    kmeans_results.append(chunk)\n",
        "\n",
        "# Concatenate all chunks\n",
        "final_kmeans_df = pd.concat(kmeans_results, ignore_index=True)\n",
        "final_kmeans_df.to_csv('/content/drive/MyDrive/kmeans_anomaly_output.csv', index=False)\n",
        "\n",
        "# Show summary\n",
        "print(\"\\nðŸ“Š K-Means Anomaly Detection Results:\")\n",
        "print(final_kmeans_df['kmeans_anomaly'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Create binary labels (if not already)\n",
        "final_kmeans_df['binary_anomaly'] = final_kmeans_df['anomaly'].map({'Normal': 0, 'Anomaly': 1})\n",
        "final_kmeans_df['kmeans_binary'] = final_kmeans_df['kmeans_anomaly'].map({'Normal': 0, 'Anomaly': 1})\n",
        "\n",
        "# Classification report\n",
        "print(\"ðŸ“ˆ Classification Report: K-Means vs Ground Truth\")\n",
        "print(classification_report(final_kmeans_df['binary_anomaly'], final_kmeans_df['kmeans_binary']))\n"
      ],
      "metadata": {
        "id": "khl_DNy8bJvD",
        "outputId": "aa74dd3f-af78-481e-aa91-12813eabf0b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "khl_DNy8bJvD",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“ˆ Classification Report: K-Means vs Ground Truth\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.90      0.95   2771314\n",
            "           1       0.14      0.77      0.23     56562\n",
            "\n",
            "    accuracy                           0.90   2827876\n",
            "   macro avg       0.57      0.83      0.59   2827876\n",
            "weighted avg       0.98      0.90      0.93   2827876\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunked LOF Implementation (500k per batch)\n",
        "\n",
        "# ðŸ“¦ Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# âœ… Reload the dataset if needed\n",
        "df = pd.read_csv('/content/drive/MyDrive/anomaly_detection_output.csv')\n",
        "df.columns = df.columns.str.strip()\n",
        "df['binary_anomaly'] = df['anomaly'].map({'Normal': 0, 'Anomaly': 1})\n",
        "\n",
        "# Parameters\n",
        "chunk_size = 500_000\n",
        "n_neighbors = 20\n",
        "contamination = 0.02\n",
        "\n",
        "# Store predictions\n",
        "lof_preds = []\n",
        "\n",
        "# Loop over chunks\n",
        "for i in range(0, len(df), chunk_size):\n",
        "    chunk = df.iloc[i:i+chunk_size].copy()\n",
        "    features = chunk.drop(columns=['anomaly', 'binary_anomaly'], errors='ignore')\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(features)\n",
        "\n",
        "    # Apply LOF\n",
        "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n",
        "    pred = lof.fit_predict(X_scaled)\n",
        "    pred = np.where(pred == -1, 1, 0)  # Convert to 1 for anomaly, 0 for normal\n",
        "\n",
        "    lof_preds.extend(pred)\n",
        "\n",
        "# Final prediction list\n",
        "df['lof_anomaly'] = lof_preds\n",
        "\n",
        "# âœ… Evaluate\n",
        "print(\"ðŸ“Š LOF Chunked Evaluation:\")\n",
        "print(df['lof_anomaly'].value_counts())\n",
        "\n",
        "print(\"\\nðŸ“ˆ Classification Report: LOF vs Ground Truth\")\n",
        "print(classification_report(df['binary_anomaly'], df['lof_anomaly']))\n"
      ],
      "metadata": {
        "id": "6IKoYdxzdahE"
      },
      "id": "6IKoYdxzdahE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}