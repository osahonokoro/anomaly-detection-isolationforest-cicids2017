{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f8b673ca-f40e-4972-a62c-63130888673b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8b673ca-f40e-4972-a62c-63130888673b",
        "outputId": "41aed39d-1ac4-46f9-82e9-7bfbf508ffa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Reloaded dataset. Shape: (2827876, 82)\n",
            "binary_anomaly\n",
            "0    2771314\n",
            "1      56562\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# ✅ Mount Google Drive first (run this only once after reconnecting)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ Load the anomaly-labeled dataset from your Drive\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/anomaly_detection_output.csv')\n",
        "df.columns = df.columns.str.strip()\n",
        "df['binary_anomaly'] = df['anomaly'].map({'Normal': 0, 'Anomaly': 1})\n",
        "\n",
        "print(\"✅ Reloaded dataset. Shape:\", df.shape)\n",
        "print(df['binary_anomaly'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  One-Class SVM with Chunked Processing (100,000 rows per batch)\n",
        "\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Parameters\n",
        "chunk_size = 100_000\n",
        "svm_preds = []\n",
        "binary_labels = []\n",
        "\n",
        "# Drop non-feature columns\n",
        "base_features = df.drop(columns=['anomaly', 'binary_anomaly'], errors='ignore')\n",
        "\n",
        "# Loop over chunks\n",
        "for i in range(0, len(df), chunk_size):\n",
        "    chunk = df.iloc[i:i+chunk_size].copy()\n",
        "    features = chunk.drop(columns=['anomaly', 'binary_anomaly'], errors='ignore')\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    scaled = scaler.fit_transform(features)\n",
        "\n",
        "    # Train One-Class SVM on the chunk\n",
        "    svm = OneClassSVM(nu=0.02, kernel='rbf', gamma='scale')\n",
        "    pred = svm.fit_predict(scaled)\n",
        "\n",
        "    # Convert predictions to readable labels\n",
        "    pred = pd.Series(pred).map({1: 'Normal', -1: 'Anomaly'})\n",
        "    svm_preds.extend(pred)\n",
        "\n",
        "    # Store true labels\n",
        "    binary_labels.extend(chunk['binary_anomaly'])\n",
        "\n",
        "    print(f\"✅ Processed rows {i} to {i+chunk_size-1}\")\n",
        "\n",
        "# Final Evaluation\n",
        "print(\"\\n📊 One-Class SVM Chunked Evaluation:\")\n",
        "print(pd.Series(svm_preds).value_counts())\n",
        "print(\"\\n📈 Classification Report:\")\n",
        "print(classification_report(binary_labels, pd.Series(svm_preds).map({'Normal': 0, 'Anomaly': 1})))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WbqMH9tMvFi",
        "outputId": "da67c428-4eb5-4db2-8b3c-9a992282e83a"
      },
      "id": "4WbqMH9tMvFi",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Processed rows 0 to 99999\n",
            "✅ Processed rows 100000 to 199999\n",
            "✅ Processed rows 200000 to 299999\n",
            "✅ Processed rows 300000 to 399999\n",
            "✅ Processed rows 400000 to 499999\n",
            "✅ Processed rows 500000 to 599999\n",
            "✅ Processed rows 600000 to 699999\n",
            "✅ Processed rows 700000 to 799999\n",
            "✅ Processed rows 800000 to 899999\n",
            "✅ Processed rows 900000 to 999999\n",
            "✅ Processed rows 1000000 to 1099999\n",
            "✅ Processed rows 1100000 to 1199999\n",
            "✅ Processed rows 1200000 to 1299999\n",
            "✅ Processed rows 1300000 to 1399999\n",
            "✅ Processed rows 1400000 to 1499999\n",
            "✅ Processed rows 1500000 to 1599999\n",
            "✅ Processed rows 1600000 to 1699999\n",
            "✅ Processed rows 1700000 to 1799999\n",
            "✅ Processed rows 1800000 to 1899999\n",
            "✅ Processed rows 1900000 to 1999999\n",
            "✅ Processed rows 2000000 to 2099999\n",
            "✅ Processed rows 2100000 to 2199999\n",
            "✅ Processed rows 2200000 to 2299999\n",
            "✅ Processed rows 2300000 to 2399999\n",
            "✅ Processed rows 2400000 to 2499999\n",
            "✅ Processed rows 2500000 to 2599999\n",
            "✅ Processed rows 2600000 to 2699999\n",
            "✅ Processed rows 2700000 to 2799999\n",
            "✅ Processed rows 2800000 to 2899999\n",
            "\n",
            "📊 One-Class SVM Chunked Evaluation:\n",
            "Normal     2771051\n",
            "Anomaly      56825\n",
            "Name: count, dtype: int64\n",
            "\n",
            "📈 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99   2771314\n",
            "           1       0.41      0.42      0.41     56562\n",
            "\n",
            "    accuracy                           0.98   2827876\n",
            "   macro avg       0.70      0.70      0.70   2827876\n",
            "weighted avg       0.98      0.98      0.98   2827876\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f481837e-c42f-4a06-b625-527814e178b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f481837e-c42f-4a06-b625-527814e178b1",
        "outputId": "b5922626-e1b9-4354-a47d-e107b56cd6e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📌 Smallest cluster assumed to be anomaly: Cluster 1\n",
            "\n",
            "📊 K-Means Anomaly Detection Results:\n",
            "kmeans_anomaly\n",
            "Normal     2507971\n",
            "Anomaly     319905\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# K-Means with Chunked Processing (500,000 rows per batch)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set paths\n",
        "input_file = '/content/drive/MyDrive/anomaly_detection_output.csv'\n",
        "chunk_size = 500_000\n",
        "\n",
        "# Placeholder for results\n",
        "kmeans_results = []\n",
        "\n",
        "# First, fit the scaler on a small subset\n",
        "sample_df = pd.read_csv(input_file, nrows=100_000)\n",
        "sample_df.columns = sample_df.columns.str.strip()\n",
        "base_sample = sample_df.drop(columns=['anomaly', 'binary_anomaly'], errors='ignore')\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(base_sample)\n",
        "\n",
        "# Fit KMeans on the same subset to get initial clusters\n",
        "scaled_sample = scaler.transform(base_sample)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "kmeans.fit(scaled_sample)\n",
        "\n",
        "# Find the smallest cluster (used as anomaly class)\n",
        "cluster_counts = Counter(kmeans.labels_)\n",
        "smallest_cluster = min(cluster_counts, key=cluster_counts.get)\n",
        "print(f\"📌 Smallest cluster assumed to be anomaly: Cluster {smallest_cluster}\")\n",
        "\n",
        "# Process in chunks\n",
        "for chunk in pd.read_csv(input_file, chunksize=chunk_size):\n",
        "    chunk.columns = chunk.columns.str.strip()\n",
        "    features = chunk.drop(columns=['anomaly', 'binary_anomaly'], errors='ignore')\n",
        "\n",
        "    # Scale\n",
        "    scaled_chunk = scaler.transform(features)\n",
        "\n",
        "    # Predict clusters\n",
        "    chunk_clusters = kmeans.predict(scaled_chunk)\n",
        "\n",
        "    # Mark anomalies\n",
        "    chunk['kmeans_anomaly'] = ['Anomaly' if label == smallest_cluster else 'Normal' for label in chunk_clusters]\n",
        "\n",
        "    # Store results\n",
        "    kmeans_results.append(chunk)\n",
        "\n",
        "# Concatenate all chunks\n",
        "final_kmeans_df = pd.concat(kmeans_results, ignore_index=True)\n",
        "final_kmeans_df.to_csv('/content/drive/MyDrive/kmeans_anomaly_output.csv', index=False)\n",
        "\n",
        "# Show summary\n",
        "print(\"\\n📊 K-Means Anomaly Detection Results:\")\n",
        "print(final_kmeans_df['kmeans_anomaly'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Create binary labels (if not already)\n",
        "final_kmeans_df['binary_anomaly'] = final_kmeans_df['anomaly'].map({'Normal': 0, 'Anomaly': 1})\n",
        "final_kmeans_df['kmeans_binary'] = final_kmeans_df['kmeans_anomaly'].map({'Normal': 0, 'Anomaly': 1})\n",
        "\n",
        "# Classification report\n",
        "print(\"📈 Classification Report: K-Means vs Ground Truth\")\n",
        "print(classification_report(final_kmeans_df['binary_anomaly'], final_kmeans_df['kmeans_binary']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khl_DNy8bJvD",
        "outputId": "aa74dd3f-af78-481e-aa91-12813eabf0b4"
      },
      "id": "khl_DNy8bJvD",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📈 Classification Report: K-Means vs Ground Truth\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.90      0.95   2771314\n",
            "           1       0.14      0.77      0.23     56562\n",
            "\n",
            "    accuracy                           0.90   2827876\n",
            "   macro avg       0.57      0.83      0.59   2827876\n",
            "weighted avg       0.98      0.90      0.93   2827876\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunked LOF Implementation (100k per batch)\n",
        "\n",
        "# 📦 Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ✅ Reload the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/anomaly_detection_output.csv')\n",
        "df.columns = df.columns.str.strip()\n",
        "df['binary_anomaly'] = df['anomaly'].map({'Normal': 0, 'Anomaly': 1})\n",
        "\n",
        "# 🔧 Parameters\n",
        "chunk_size = 100_000  # Reduced chunk size\n",
        "n_neighbors = 20\n",
        "contamination = 0.02\n",
        "\n",
        "# 📥 Store predictions\n",
        "lof_preds = []\n",
        "\n",
        "# 🔁 Loop over chunks\n",
        "for i in range(0, len(df), chunk_size):\n",
        "    print(f\"Processing chunk {i} to {min(i + chunk_size, len(df))}...\")\n",
        "\n",
        "    chunk = df.iloc[i:i+chunk_size].copy()\n",
        "    features = chunk.drop(columns=['anomaly', 'binary_anomaly'], errors='ignore')\n",
        "\n",
        "    # ✅ Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(features)\n",
        "\n",
        "    # 🧠 Apply LOF\n",
        "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n",
        "    pred = lof.fit_predict(X_scaled)\n",
        "    pred = np.where(pred == -1, 1, 0)  # 1 = Anomaly, 0 = Normal\n",
        "\n",
        "    lof_preds.extend(pred)\n",
        "\n",
        "# 📊 Final result\n",
        "df = df.iloc[:len(lof_preds)]  # Ensure alignment\n",
        "df['lof_anomaly'] = lof_preds\n",
        "\n",
        "# 🧾 Evaluate\n",
        "print(\"\\n📊 LOF Chunked Evaluation:\")\n",
        "print(df['lof_anomaly'].value_counts())\n",
        "\n",
        "print(\"\\n📈 Classification Report: LOF vs Ground Truth\")\n",
        "print(classification_report(df['binary_anomaly'], df['lof_anomaly']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IKoYdxzdahE",
        "outputId": "5a51554b-525a-4dec-b587-bc8374bddd92"
      },
      "id": "6IKoYdxzdahE",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 0 to 100000...\n",
            "Processing chunk 100000 to 200000...\n",
            "Processing chunk 200000 to 300000...\n",
            "Processing chunk 300000 to 400000...\n",
            "Processing chunk 400000 to 500000...\n",
            "Processing chunk 500000 to 600000...\n",
            "Processing chunk 600000 to 700000...\n",
            "Processing chunk 700000 to 800000...\n",
            "Processing chunk 800000 to 900000...\n",
            "Processing chunk 900000 to 1000000...\n",
            "Processing chunk 1000000 to 1100000...\n",
            "Processing chunk 1100000 to 1200000...\n",
            "Processing chunk 1200000 to 1300000...\n",
            "Processing chunk 1300000 to 1400000...\n",
            "Processing chunk 1400000 to 1500000...\n",
            "Processing chunk 1500000 to 1600000...\n",
            "Processing chunk 1600000 to 1700000...\n",
            "Processing chunk 1700000 to 1800000...\n",
            "Processing chunk 1800000 to 1900000...\n",
            "Processing chunk 1900000 to 2000000...\n",
            "Processing chunk 2000000 to 2100000...\n",
            "Processing chunk 2100000 to 2200000...\n",
            "Processing chunk 2200000 to 2300000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_lof.py:322: UserWarning: Duplicate values are leading to incorrect results. Increase the number of neighbors for more accurate results.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 2300000 to 2400000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_lof.py:322: UserWarning: Duplicate values are leading to incorrect results. Increase the number of neighbors for more accurate results.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 2400000 to 2500000...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neighbors/_lof.py:322: UserWarning: Duplicate values are leading to incorrect results. Increase the number of neighbors for more accurate results.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 2500000 to 2600000...\n",
            "Processing chunk 2600000 to 2700000...\n",
            "Processing chunk 2700000 to 2800000...\n",
            "Processing chunk 2800000 to 2827876...\n",
            "\n",
            "📊 LOF Chunked Evaluation:\n",
            "lof_anomaly\n",
            "0    2771318\n",
            "1      56558\n",
            "Name: count, dtype: int64\n",
            "\n",
            "📈 Classification Report: LOF vs Ground Truth\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98   2771314\n",
            "           1       0.02      0.02      0.02     56562\n",
            "\n",
            "    accuracy                           0.96   2827876\n",
            "   macro avg       0.50      0.50      0.50   2827876\n",
            "weighted avg       0.96      0.96      0.96   2827876\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}